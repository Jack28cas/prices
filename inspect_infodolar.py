import requests
import re
from bs4 import BeautifulSoup
import json

def inspeccionar_infodolar():
    """
    Inspecciona la p√°gina de InfoDolar para buscar llamadas AJAX/API
    """
    url = "https://www.infodolar.com/cotizacion-dolar-blue.aspx"
    
    # Headers para simular un navegador
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print("üîç Inspeccionando InfoDolar...")
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            print(f"‚úÖ P√°gina cargada correctamente (Status: {response.status_code})")
            
            # Buscar en el HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar scripts que puedan contener llamadas AJAX
            scripts = soup.find_all('script')
            
            print(f"\nüìú Analizando {len(scripts)} scripts...")
            
            # Patrones para buscar URLs de API
            api_patterns = [
                r'ajax\s*\(\s*[\'"]([^\'"]+)[\'"]',
                r'fetch\s*\(\s*[\'"]([^\'"]+)[\'"]',
                r'XMLHttpRequest.*open\s*\(\s*[\'"][^\'"]++[\'"],\s*[\'"]([^\'"]+)[\'"]',
                r'\.get\s*\(\s*[\'"]([^\'"]+)[\'"]',
                r'\.post\s*\(\s*[\'"]([^\'"]+)[\'"]',
                r'api[/\w]*\.aspx',
                r'[\'"]([^\'"]*api[^\'"]*\.aspx?[^\'"]*)[\'"]',
                r'[\'"]([^\'"]*ajax[^\'"]*\.aspx?[^\'"]*)[\'"]',
                r'[\'"]([^\'"]*data[^\'"]*\.aspx?[^\'"]*)[\'"]'
            ]
            
            found_urls = set()
            
            for script in scripts:
                if script.string:
                    script_content = script.string
                    
                    # Buscar patrones de API
                    for pattern in api_patterns:
                        matches = re.findall(pattern, script_content, re.IGNORECASE)
                        for match in matches:
                            if 'infodolar' in match or match.startswith('/') or match.startswith('http'):
                                found_urls.add(match)
            
            # Buscar en atributos de elementos
            print("\nüîç Buscando en atributos de elementos...")
            
            # Buscar data-* attributes
            elements_with_data = soup.find_all(attrs={'data-url': True})
            elements_with_data += soup.find_all(attrs={'data-api': True})
            elements_with_data += soup.find_all(attrs={'data-endpoint': True})
            
            for element in elements_with_data:
                for attr, value in element.attrs.items():
                    if 'url' in attr.lower() or 'api' in attr.lower():
                        found_urls.add(value)
            
            # Mostrar URLs encontradas
            if found_urls:
                print(f"\nüéØ URLs potenciales encontradas:")
                for url in sorted(found_urls):
                    print(f"   ‚Ä¢ {url}")
                    
                # Probar las URLs encontradas
                print(f"\nüß™ Probando URLs encontradas...")
                for url in found_urls:
                    probar_url(url, headers)
            else:
                print("\n‚ùå No se encontraron URLs de API evidentes")
                
            # Buscar patrones espec√≠ficos de InfoDolar
            print(f"\nüîç Buscando patrones espec√≠ficos...")
            
            # Buscar referencias a cotizaciones
            cotiz_pattern = r'cotiz[a-z]*[\'"]?\s*:\s*[\'"]?([^\'"]+)[\'"]?'
            cotiz_matches = re.findall(cotiz_pattern, response.text, re.IGNORECASE)
            
            if cotiz_matches:
                print("üí∞ Referencias a cotizaciones encontradas:")
                for match in cotiz_matches[:5]:  # Mostrar solo las primeras 5
                    print(f"   ‚Ä¢ {match}")
                    
        else:
            print(f"‚ùå Error al cargar la p√°gina (Status: {response.status_code})")
            
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")

def probar_url(url, headers):
    """
    Prueba una URL para ver si devuelve JSON
    """
    try:
        # Construir URL completa si es relativa
        if url.startswith('/'):
            url = 'https://www.infodolar.com' + url
        elif not url.startswith('http'):
            url = 'https://www.infodolar.com/' + url
            
        print(f"   üåê Probando: {url}")
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            try:
                # Intentar parsear como JSON
                data = response.json()
                print(f"   ‚úÖ JSON encontrado! Claves: {list(data.keys()) if isinstance(data, dict) else 'Lista'}")
                
                # Mostrar un preview del JSON
                json_preview = json.dumps(data, indent=2)[:200] + "..."
                print(f"   üìÑ Preview: {json_preview}")
                
            except json.JSONDecodeError:
                # No es JSON, pero ver si contiene datos √∫tiles
                content = response.text[:200]
                if any(keyword in content.lower() for keyword in ['dolar', 'blue', 'cotiz', 'precio']):
                    print(f"   üìÑ Contenido relevante encontrado: {content}...")
                else:
                    print(f"   ‚ö†Ô∏è  Respuesta no JSON")
        else:
            print(f"   ‚ùå Status: {response.status_code}")
            
    except Exception as e:
        print(f"   ‚ùå Error: {str(e)}")

if __name__ == "__main__":
    inspeccionar_infodolar() 